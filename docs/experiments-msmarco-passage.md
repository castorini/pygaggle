# PyGaggle: Neural Baselines on [MS MARCO Passage Retrieval](https://github.com/microsoft/MSMARCO-Passage-Ranking)

This page contains instructions for running various neural reranking baselines on the MS MARCO *passage* ranking task. 
Note that there is also a separate [MS MARCO *document* ranking task](experiments-msmarco-doc.md).
Make sure that you have access to a GPU before running this.

Prior to running this, we suggest looking at our first-stage [BM25 ranking instructions](https://github.com/castorini/anserini/blob/master/docs/experiments-msmarco-passage.md).
We rerank the BM25 run files that contain ~ 1000 passages per query using both monoBERT and monoT5.

Keeping computational resources in mind, our instructions primarily focus on a 105 query subset of the MS MARCO dev set. 
Running these instructions with the entire MS MARCO dev set should give about the same results as that in the corresponding paper. 

*Note: Run the following instructions at root of this repo. Installation must have been done from source.*

## Models

+ monoBERT-Large: Passage Re-ranking with BERT [(Nogueira et al., 2019)](https://arxiv.org/pdf/1901.04085.pdf)
+ monoT5-base: Document Ranking with a Pretrained Sequence-to-Sequence Model [(Nogueira et al., 2020)](https://arxiv.org/pdf/2003.06713.pdf)

## Data Prep

We're first going to download the queries, qrels and run files corresponding to the MS MARCO set considered. The run file is generated by following the BM25 ranking instructions. We'll store all these files in the `data` directory.

```
wget https://www.dropbox.com/s/5xa5vjbjle0c8jv/msmarco_ans_small.zip -P data
```

To confirm, `msmarco_ans_small.zip` should have MD5 checksum of `65d8007bfb2c72b5fc384738e5572f74`.

Next, we extract the contents of the zip file into runs. 

```
unzip msmarco_ans_small.zip -d data
```

We can evaluate the first-stage retrieved documents using the official MS MARCO evaluation script.

```
python evaluate/msmarco/msmarco_eval.py data/msmarco_ans_small/qrels.dev.small.tsv data/msmarco_ans_small/run.dev.small.tsv
```

And the output should be:

```
#####################
MRR @10: 0.15906651549508694
QueriesRanked: 105
#####################
```

Let's download and extract the pre-built MS MARCO index into the `indexes` directory:

```
wget https://git.uwaterloo.ca/jimmylin/anserini-indexes/raw/master/index-msmarco-passage-20191117-0ed488.tar.gz -P indexes
tar xvfz indexes/index-msmarco-passage-20191117-0ed488.tar.gz -C indexes
```

## Model Prep

Let's download and extract monoBERT into the `models` directory

```
wget https://www.dropbox.com/s/jr0hpksboh7pa48/monobert_msmarco_large.zip -P models
unzip models/monobert_msmarco_large.zip -d models
```

While running the re-ranking script with the monoT5 model, it is automatically downloaded from Google Cloud Storage. 

Now, we can begin with re-ranking the set.

## Re-Ranking with monoBERT

First, lets evaluate using monoBERT!

```
python -um pygaggle.run.evaluate_passage_ranker --split dev \
	                                            --method seq_class_transformer \
	                                            --model-name-or-path models/monobert_msmarco_large \
	                                            --data-dir data/msmarco_ans_small/ \
	                                            --index-dir indexes/index-msmarco-passage-20191117-0ed488 \
	                                            --dataset msmarco \
	                                            --output-file runs/run.monobert.ans_small.dev.tsv
```

Upon completion, the following output will be visible:

```
precision@1	0.2761904761904762
recall@3	0.42698412698412697
recall@50	0.8174603174603176
recall@1000	0.8476190476190476
mrr	0.41089693612003686
mrr@10	0.4026795162509449
```

It takes about ~ 52 minutes to re-rank this subset on MS MARCO using a P100. 
The type of GPU will directly influence your inference time. 
It is possible that the default batch results in a GPU OOM error.
In this case, assigning a batch size (using option `--batch-size`) which is smaller than the default (96) should help!

The re-ranked run file `run.monobert.ans_small.dev.tsv` will also be available in the `runs` directory upon completion. 
We can use this to verify that the MRR@10 is indeed right using the official MS MARCO evaluation script:

```
python evaluate/msmarco/msmarco_eval.py data/msmarco_ans_small/qrels.dev.small.tsv runs/run.monobert.ans_small.dev.tsv
```

You should see the same result. Great, let's move on to monoT5!

## Re-Ranking with monoT5

We use the monoT5-base variant as it is the easiest to run without access to larger GPUs/TPUs. Let us now re-rank the set:

```
python -um pygaggle.run.evaluate_passage_ranker --split dev \
                                                --method t5 \
                                                --model-name-or-path gs://neuralresearcher_data/doc2query/experiments/367 \
                                                --data-dir data/msmarco_ans_small \
                                                --model-type t5-base \
                                                --dataset msmarco \
                                                --index-dir indexes/index-msmarco-passage-20191117-0ed488 \
                                                --batch-size 32 \
                                                --output-file runs/run.monot5.ans_small.dev.tsv
```

The following output will be visible after it has finished:

```
precision@1	0.26666666666666666
recall@3	0.4603174603174603
recall@50	0.8063492063492063
recall@1000	0.8476190476190476
mrr	0.3973368360121561
mrr@10	0.39044217687074834
```

It takes about ~ 13 minutes to re-rank this subset on MS MARCO using a P100. 
It is worth noting again that you might need to modify the batch size to best fit the GPU at hand.

Upon completion, the re-ranked run file `run.monot5.ans_small.dev.tsv` will be available in the `runs` directory.


Awesome, we can verify that this MRR@10 is indeed right using the official MS MARCO evaluation script:

```
python evaluate/msmarco/msmarco_eval.py data/msmarco_ans_small/qrels.dev.small.tsv runs/run.monot5.ans_small.dev.tsv
```

You should see the same result.

If you were able to replicate any of these results, please submit a PR adding to the replication log!


## Replication Log

### monoBERT

### monoT5

