# PyGaggle: Neural Ranking Baselines on [MS MARCO Passage Retrieval](https://github.com/microsoft/MSMARCO-Passage-Ranking) - Dev Subset 

This page contains instructions for running various neural reranking baselines on the MS MARCO *passage* ranking task. 
Note that there is also a separate [MS MARCO *document* ranking task](https://github.com/castorini/anserini/blob/master/docs/experiments-msmarco-doc.md).

Prior to running this, we suggest looking at our first-stage [BM25 ranking instructions](https://github.com/castorini/anserini/blob/master/docs/experiments-msmarco-passage.md).
We rerank the BM25 run files that contain ~1000 passages per query using both monoBERT and monoT5. 
monoBERT and monoT5 are pointwise rerankers. This means that each document is scored independently using either BERT or T5 respectively.

Since it can take many hours to run these models on all of the 6980 queries from the MS MARCO dev set, we will instead use a subset of 105 queries randomly sampled from the dev set. 
Running these instructions with the entire MS MARCO dev set should give about the same results as that in the corresponding paper. 

Note 1: Run the following instructions at root of this repo.
Note 2: Make sure that you have access to a GPU
Note 3: Installation must have been done from source and make sure the [anserini-eval](https://github.com/castorini/anserini-eval) submodule is pulled. 
To do this, first clone the repository recursively.

```
git clone --recursive https://github.com/castorini/pygaggle.git
```

Then install PyGaggle using:

```
pip install pygaggle/
```

## Models

+ monoBERT-Large: Passage Re-ranking with BERT [(Nogueira et al., 2019)](https://arxiv.org/pdf/1901.04085.pdf)
+ monoT5-base: Document Ranking with a Pretrained Sequence-to-Sequence Model [(Nogueira et al., 2020)](https://arxiv.org/pdf/2003.06713.pdf)

## Data Prep

We're first going to download the queries, qrels and run files corresponding to the MS MARCO set considered. The run file is generated by following the BM25 ranking instructions. We'll store all these files in the `data` directory.

```
wget https://www.dropbox.com/s/5xa5vjbjle0c8jv/msmarco_ans_small.zip -P data
```

To confirm, `msmarco_ans_small.zip` should have MD5 checksum of `65d8007bfb2c72b5fc384738e5572f74`.

Next, we extract the contents into `data`. 

```
unzip data/msmarco_ans_small.zip -d data
```

We should have these files in `data/msmarco_ans_small/`
```
ls data/msmarco_ans_small -1
qrels.dev.small.tsv
queries.dev.small.tsv
run.dev.small.tsv
scores
``` 

Let's also download MS MARCO passage dataset to visualize the actual passages after re-ranking.
```
mkdir collections/msmarco-passage

wget https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz -P collections/msmarco-passage

tar xvfz collections/msmarco-passage/collectionandqueries.tar.gz -C collections/msmarco-passage
```

As a sanity check, we can evaluate the first-stage retrieved documents using the official MS MARCO evaluation script.

```
python tools/scripts/msmarco/msmarco_passage_eval.py data/msmarco_ans_small/qrels.dev.small.tsv data/msmarco_ans_small/run.dev.small.tsv
```

The output should be:

```
#####################
MRR @10: 0.15906651549508694
QueriesRanked: 105
#####################
```

<details>
<summary>What's going on here?</summary>

If you peak inside the `data/msmarco_ans_small/run.dev.small.tsv` file
```
head -5 data/msmarco_ans_small/run.dev.small.tsv
188714  2133570 1
188714  4321742 2
188714  4321745 3
188714  8523352 4
188714  3573129 5
```

you will notice that the first column is the `qid` corresponding to a query from `data/msmarco_ans_small/queries.dev.small.tsv` and the second column is the `docid` of the retrieved result (i.e., the hit), and the third column is the rank position. That is, in a search interface, for `qid` 188714 `docid` 2133570 would be shown in the top position, `docid` 4321742 would be shown in the second position, etc.

Now, let's see the actual query with `qid` 188714
```
grep 188714 data/msmarco_ans_small/qrels.dev.small.tsv
188714  foods and supplements to lower blood sugar
```


Let's see the passage text of the first hit by grepping `docid` 2133570
```
grep 2133570 collections/msmarco-passage/collection.tsv
2133570 A healthy diet is essential to reversing prediabetes. There are no foods, herbs, drinks, or supplements that lower blood sugar. Only medication and exercise can. But there are things you can eat and drink that are low on the glycemic index (GI). This means these foods wonât raise your blood sugar and may help you avoid a blood sugar spike.
```
Let's verify if `docid` 2133570 is actually a relevant hit to our query (`qid` 188714) by checking the `data/msmarco_ans_small/qrels.dev.small.tsv` generated by human annotators

```
grep 188714 collections/msmarco-passage/qrels.dev.small.tsv  
188714  0       8003843 1
188714  0       4321745 1
188714  0       8003849 1
```

Recall that in a `qrel` file, the first column is the `qid` of a certain query, the third is the `docid` of a passage, and the last column is whether or not the `docid` is a hit to the `qid`(`1` is a hit and `0` is not). In this case, notice that `docid` 2133570 does not appear in the third column of the passage hits for `qid` 188714, thus it is not a relevant passage that should be displayed to the user, especially at the top location!

We will later see if re-ranking using MonoBert and MonoT5 has helped with improving our hit rankings.
</details>
 </br>

Let's download and extract the pre-built MS MARCO index into `indexes`:

```
wget https://git.uwaterloo.ca/jimmylin/anserini-indexes/raw/master/index-msmarco-passage-20191117-0ed488.tar.gz -P indexes
tar xvfz indexes/index-msmarco-passage-20191117-0ed488.tar.gz -C indexes
```

Now, we can begin with re-ranking the set.

## Re-Ranking with monoBERT

First, lets evaluate using monoBERT!

```
python -um pygaggle.run.evaluate_passage_ranker --split dev \
                                                --method seq_class_transformer \
                                                --model castorini/monobert-large-msmarco \
                                                --dataset data/msmarco_ans_small/ \
                                                --index-dir indexes/index-msmarco-passage-20191117-0ed488 \
                                                --task msmarco \
                                                --output-file runs/run.monobert.ans_small.dev.tsv
```

Upon completion, the following output will be visible:

```
precision@1     0.2761904761904762
recall@3        0.42698412698412697
recall@50       0.8174603174603176
recall@1000     0.8476190476190476
mrr     0.41089693612003686
mrr@10  0.4026795162509449
```

It takes about ~52 minutes to re-rank this subset on MS MARCO using a P100. 
The type of GPU will directly influence your inference time. 
It is possible that the default batch results in a GPU OOM error.
In this case, assigning a batch size (using option `--batch-size`) which is smaller than the default (96) should help!

The re-ranked run file `run.monobert.ans_small.dev.tsv` will also be available in the `runs` directory upon completion.

<details>
<summary>What's going on here?</summary>

If you peak inside the generated `runs/run.monobert.ans_small.dev.tsv`
```
head -5 runs/run.monobert.ans_small.dev.tsv
188714  4321745 1
188714  6301923 2
188714  6442308 3
188714  1051360 4
188714  4816868 5
```
you will notice that the first column is the `qid` corresponding to a query from `data/msmarco_ans_small/queries.dev.small.tsv` and the second column is the `docid` of the retrieved result (i.e., the hit), and the third column is the rank position. That is, in a search interface, for `qid` 188714 `docid` 4321745 would be shown in the top position, `docid` 6301923 would be shown in the second position, etc.

Now, let's see the actual query with `qid` 188714
```
grep 188714 data/msmarco_ans_small/qrels.dev.small.tsv
188714  foods and supplements to lower blood sugar
```

let's also see the passage text of the first hit by grepping `docid` 4321745
```
grep 4321745 collections/msmarco-passage/collection.tsv
4321745 Food And Supplements That Lower Blood Sugar Levels. Cinnamon: Researchers are finding that cinnamon reduces blood sugar levels naturally when taken daily. If you absolutely love cinnamon you can sprinkle the recommended six grams of cinnamon on your food throughout the day to achieve the desired effect.
```
In this case, the passage seems relevant to the query. Let's now compare this passage with the top passage hit from the original `data/msmarco_ans_small/run.dev.small.tsv`run file. Grep the first passage hit for `qid` 188714

```
grep 188714 data/msmarco_ans_small/run.dev.small.tsv | head -1
188714  2133570 1
```

Now, let's grep the passage with `docid` 2133570
```
grep 2133570 collections/msmarco-passage/collection.tsv 
2133570 A healthy diet is essential to reversing prediabetes. There are no foods, herbs, drinks, or supplements that lower blood sugar. Only medication and exercise can. But there are things you can eat and drink that are low on the glycemic index (GI). This means these foods wonât raise your blood sugar and may help you avoid a blood sugar spike.
```

Notice that the top hit(`docid` 4321745) from the MonoBert re-ranked run file seems more relevant to the query with `qid` 188714. Let's verify if `docid` 4321745 is actually a relevant hit to our query (`qid` 188714) by checking the `data/msmarco_ans_small/qrels.dev.small.tsv` generated by human annotators

```
grep 188714 collections/msmarco-passage/qrels.dev.small.tsv  
188714  0       8003843 1
188714  0       4321745 1
188714  0       8003849 1
```

Recall that in a `qrel` file, the first column is the `qid` of a certain query, the third is the `docid` of a passage, and the last column is whether or not the `docid` is a hit to the `qid`(`1` is a hit and `0` is not). In this case, notice that `docid` 4321745 does appear in the third column of the passage hits relevant to `qid` 188714, thus it is a relevant passage that should be displayed to the user, unlike `docid` 2133570 (the top hit from the original run file) which does not appear at all as a relevant passage to `qid` 188714.


Thus, re-ranking with MonoBert certainly improved the top hit results.
</details>
 </br>

We can use the official MS MARCO evaluation script to verify the MRR@10:

```
python tools/scripts/msmarco/msmarco_passage_eval.py data/msmarco_ans_small/qrels.dev.small.tsv runs/run.monobert.ans_small.dev.tsv
```

You should see the same result. Great, let's move on to monoT5!

## Re-Ranking with monoT5

We use the monoT5-base variant as it is the easiest to run without access to larger GPUs/TPUs. Let us now re-rank the set:

```
python -um pygaggle.run.evaluate_passage_ranker --split dev \
                                                --method t5 \
                                                --model castorini/monot5-base-msmarco \
                                                --dataset data/msmarco_ans_small \
                                                --model-type t5-base \
                                                --task msmarco \
                                                --index-dir indexes/index-msmarco-passage-20191117-0ed488 \
                                                --batch-size 32 \
                                                --output-file runs/run.monot5.ans_small.dev.tsv
```

The following output will be visible after it has finished:

```
precision@1     0.26666666666666666
recall@3        0.4603174603174603
recall@50       0.8063492063492063
recall@1000     0.8476190476190476
mrr     0.3973368360121561
mrr@10  0.39044217687074834
```

It takes about ~13 minutes to re-rank this subset on MS MARCO using a P100. 
It is worth noting again that you might need to modify the batch size to best fit the GPU at hand.

Upon completion, the re-ranked run file `run.monot5.ans_small.dev.tsv` will be available in the `runs` directory.

<details>
<summary>What's going on here?</summary>

If you peak inside the generated `runs/run.monot5.ans_small.dev.tsv`
```
head -5 runs/run.monot5.ans_small.dev.tsv
188714  4321745 1
188714  1051360 2
188714  6442308 3
188714  5499899 4
188714  1022485 5
```
you will notice that the first column is the `qid` corresponding to a query from `data/msmarco_ans_small/queries.dev.small.tsv` and the second column is the `docid` of the retrieved result (i.e., the hit), and the third column is the rank position. That is, in a search interface, for `qid` 188714 `docid` 4321745 would be shown in the top position, `docid` 6301923 would be shown in the second position, etc.

Now, let's see the actual query with `qid` 188714
```
grep 188714 data/msmarco_ans_small/qrels.dev.small.tsv
188714  foods and supplements to lower blood sugar
```

let's also see the passage text of the first hit by grepping `docid` 4321745
```
grep 4321745 collections/msmarco-passage/collection.tsv
4321745 Food And Supplements That Lower Blood Sugar Levels. Cinnamon: Researchers are finding that cinnamon reduces blood sugar levels naturally when taken daily. If you absolutely love cinnamon you can sprinkle the recommended six grams of cinnamon on your food throughout the day to achieve the desired effect.
```
In this case, the passage seems relevant to the query. Let's now compare this passage with the top passage hit from the original `data/msmarco_ans_small/run.dev.small.tsv`run file. Grep the first passage hit for `qid` 188714

```
grep 188714 data/msmarco_ans_small/run.dev.small.tsv | head -1
188714  2133570 1
```

Now, let's grep the passage with `docid` 2133570
```
grep 2133570 collections/msmarco-passage/collection.tsv 
2133570 A healthy diet is essential to reversing prediabetes. There are no foods, herbs, drinks, or supplements that lower blood sugar. Only medication and exercise can. But there are things you can eat and drink that are low on the glycemic index (GI). This means these foods wonât raise your blood sugar and may help you avoid a blood sugar spike.
```

Notice that the top hit(`docid` 4321745) from the MonoT5 re-ranked run file seems more relevant to the query with `qid` 188714. Let's verify if `docid` 4321745 is actually a relevant hit to our query (`qid` 188714) by checking the `data/msmarco_ans_small/qrels.dev.small.tsv` generated by human annotators

```
grep 188714 collections/msmarco-passage/qrels.dev.small.tsv  
188714  0       8003843 1
188714  0       4321745 1
188714  0       8003849 1
```

Recall that in a `qrel` file, the first column is the `qid` of a certain query, the third is the `docid` of a passage, and the last column is whether or not the `docid` is a hit to the `qid`(`1` is a hit and `0` is not). In this case, notice that `docid` 4321745 does appear in the third column of the passage hits relevant to `qid` 188714, thus it is a relevant passage that should be displayed to the user, unlike `docid` 2133570 (the top hit from the original run file) which does not appear at all as a relevant passage to `qid` 188714.


Thus, re-ranking with MonoT5 certainly improved the top hit results.
</details>
 </br>

We can use the official MS MARCO evaluation script to verify the MRR@10:

```
python tools/scripts/msmarco/msmarco_passage_eval.py data/msmarco_ans_small/qrels.dev.small.tsv runs/run.monot5.ans_small.dev.tsv
```

You should see the same result.

If you were able to replicate these results, please submit a PR adding to the replication log!


## Replication Log

+ Results replicated by [@MXueguang](https://github.com/MXueguang) on 2020-05-22 (commit [`69de7db`](https://github.com/castorini/pygaggle/commit/69de7db843bbe9201113c4d94c9e90be36094350)) (Tesla P4)
+ Results replicated by [@richard3983](https://github.com/richard3983) on 2020-05-22 (commit [`6e9dfc6`](https://github.com/richard3983/pygaggle/commit/6e9dfc62083c15233600c41737110c9989043b98)) (Tesla P100)
+ Results replicated by [@HangCui0510](https://github.com/HangCui0510) on 2020-05-29 (commit [`591e7ff`](https://github.com/HangCui0510/pygaggle/commit/591e7ffd6cc826fd2bae5e721f9693452f9e4a49)) (Tesla P100)
+ Results replicated by [@kelvin-jiang](https://github.com/kelvin-jiang) on 2020-05-31 (commit [`82dc086`](https://github.com/HangCui0510/pygaggle/commit/82dc086b86d828147dad34d9a7f8bb66a3c23c88)) (GeForce RTX 2080 Ti)
+ Results replicated by [@justinborromeo](https://github.com/justinborromeo) on 2020-07-02 (commit [`70b2a9f`](https://github.com/castorini/pygaggle/commit/70b2a9fe625554aeae02f64eb68f1edc57f96860)) (GeForce GTX 960M)
+ Results replicated by [@mrkarezina](https://github.com/mrkarezina) on 2020-07-19 (commit [`c1a54cb`](https://github.com/castorini/pygaggle/commit/c1a54cb012a1d4ea24a2ce2bc24298417279a9c4)) (Tesla T4)
+ Results replicated by [@qguo96](https://github.com/qguo96) on 2020-09-08 (commit [`94befbd`](https://github.com/qguo96/pygaggle/commit/94befbd58b19c3e46d930e67797102bf174efd01)) (Tesla T4 on Colab)
+ Results replicated by [@yuxuan-ji](https://github.com/yuxuan-ji) on 2020-09-08 (commit[`94befbd`](https://github.com/castorini/pygaggle/commit/94befbd58b19c3e46d930e67797102bf174efd01)) (Tesla T4 on Colab)
+ Results replicated by [@LizzyZhang-tutu](https://github.com/LizzyZhang-tutu) on 2020-09-09 (commit[`8eeefa5`](https://github.com/castorini/pygaggle/commit/8eeefa578c65e2da78be129c87dfb40beb74099c)) (Tesla T4 on Colab)
+ Results replicated by [@wiltan-uw](https://github.com/wiltan-uw) on 2020-09-13 (commit[`41513a9`](https://github.com/castorini/pygaggle/commit/41513a9f496bd59523993ce134cc35a7b881e5a1)) (RTX 2070S)
+ Results replicated by [@jhuang265](https://github.com/jhuang265) on 2020-10-18 (commit[`e815051`](https://github.com/castorini/pygaggle/commit/e815051f2cee1af98b370ee030b66c07a8a287f3)) (Tesla P100 on Colab)
+ Results replicated by [@stephaniewhoo](https://github.com/stephaniewhoo) on 2020-10-25 (commit[`e815051`](https://github.com/castorini/pygaggle/commit/e815051f2cee1af98b370ee030b66c07a8a287f3)) (Tesla V100 on Compute Canada)
+ Results replicated by [@rayyang29](https://github.com/rayyang29) on 2020-11-05 (commit[`19b16d2`](https://github.com/castorini/pygaggle/commit/19b16d28b20bbcead359fc9b4086f33e5c7598f9)) (Tesla T4)
+ Results replicated by [@estella98](https://github.com/estella98) on 2020-11-10 (commit[`5e1e0dd`](https://github.com/castorini/pygaggle/commit/5e1e0dd37c71560e46e8a7f4aa1617b1affd23a7)) (Tesla T4 on Colab) 
+ Results replicated by [@rakeeb123](https://github.com/rakeeb123) on 2020-12-10 (commit[`9a1fe70`](https://github.com/castorini/pygaggle/commit/9a1fe703711011cde69cd78968cb3f00190a3144)) (GeForce 940MX and Tesla V100 on Compute Canada)
+ Results replicated by [@Dahlia-Chehata](https://github.com/Dahlia-Chehata) on 2021-01-01 (commit[`968363e`](https://github.com/castorini/pygaggle/commit/968363ee27bd3ec4d20bdf89eb5cd41e1a6410a5)) (Tesla P100 on Compute Canada)
+ Results replicated by [@KaiSun314](https://github.com/KaiSun314) on 2021-01-08 (commit[`c7fdc4f`](https://github.com/castorini/pygaggle/commit/c7fdc4f46375a05f02d62fdfd549d43cefad3537)) (Nvidia GeForce GTX 1060)
+ Results replicated by [@wongalvis14](https://github.com/wongalvis14) on 2021-02-22 (commit[`7c0ebbe`](https://github.com/castorini/pygaggle/commit/7c0ebbeb20dc867ee68d21c6ac7da84073bdb6f6)) (GeForce RTX 2080 Ti on Hydra)
+ Results replicated by [@saileshnankani](https://github.com/saileshnankani) on 2021-05-05 (commit[`95b3da7`](https://github.com/castorini/pygaggle/commit/95b3da7cf2822f3581a2b1891dd5f54b258a04e4)) (Tesla T4 on Colab)
+ Results replicated by [@andrewyguo](https://github.com/andrewyguo) on 2021-05-05 (commit[`6f0381e`](https://github.com/castorini/pygaggle/commit/6f0381e6a3f6ea5f0d284abb156c549bb4c54578)) (Tesla T4 on Colab)
+ Results replicated by [@larryli1999](https://github.com/larryli1999) on 2021-05-05 (commit[`53b77f4`](https://github.com/castorini/pygaggle/commit/53b77f4219fc990d77e94007b36be6a20678e4d2)) (Tesla T4 on Colab)
+ Results replicated by [@mzzchy](https://github.com/mzzchy) on 2021-08-29 (commit[`6b9c895`](https://github.com/castorini/pygaggle/commit/b5315e9f1f7466e689983e3c4e70134e36d4be49)) (GeForce GTX 1660 Ti)
+ Results replicated by [@AlexWang000](https://github.com/AlexWang000) on 2021-10-22 (commit[`63f92cf`](https://github.com/castorini/pygaggle/commit/63f92cf6f83a8909f4bf6528b402632d7498b8d6)) (Tesla T4 on Colab)
+ Results replicated by [@manveertamber](https://github.com/manveertamber) on 2021-12-08 (commit[`b3e11c4`](https://github.com/castorini/pygaggle/commit/b3e11c46a6cf17e0e99a8eed7de316eb0117ee19)) (GeForce GTX 1660)
+ Results replicated by [@lingwei-gu](https://github.com/lingwei-gu) on 2022-01-05 (commit [`d671f62`](https://github.com/castorini/pygaggle/commit/d671f62e4a269b5d79068f25267edd6078e568b5)) (Tesla T4 on Colab)
+ Results replicated by [@jx3yang](https://github.com/jx3yang) on 2022-05-10 (commit[`a326d49`](https://github.com/castorini/pygaggle/commit/a326d4983db6f84e4c519efa9e2dec91f776268e)) (Tesla T4 on Colab)
+ Results replicated by [@alvind1](https://github.com/alvind1) on 2022-05-12 (commit[`9d859a1`](https://github.com/castorini/pygaggle/commit/9d859a16d38e1c4281ac3c0588a4fa00e9e39e9a)) (Tesla T4 on Colab)
+ Results replicated by [@aivan6842](https://github.com/aivan6842) on 2022-08-09 (commit[`f54ae53`](https://github.com/castorini/pygaggle/commit/f54ae53d6183c1b66444fa5a0542301e0d1090f5)) (GeForce RTX 3070)